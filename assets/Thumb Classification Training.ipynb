{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phTR4tsK1NKJ"
      },
      "source": [
        "# Training of the model for Thumb Classification\n",
        "\n",
        "The goal of this notebook is to train a classification model that can detect thumb up and thumb down in video stream\n",
        "\n",
        "This notebook has been run on Google Colab to take advantage of the GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lYPj1I1xeScY"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import os \n",
        "import shutil \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input as preprocess_input_vgg\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYHyHnW66OM-"
      },
      "source": [
        "Define the three classification category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bDqwOq116Skb"
      },
      "outputs": [],
      "source": [
        "NB_CLASSES = 3\n",
        "THUMB_UP = '01'\n",
        "THUMB_DOWN = '02'\n",
        "OTHER = '03'\n",
        "PATH_TO_ZIP = '/content/DSTI_Python_Labs/assets/data_train'\n",
        "ZIP_FILE = 'thumbv3.zip'\n",
        "IMAGES_RAW = 'images_raw_thumb'\n",
        "IMAGES_SPLITED = 'images_keras_thumb'\n",
        "MODEL_FILE = 'model_thumb_v24112020.h5'\n",
        "SAVE_FOLDER = 'save_model_thumb'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoEbjHaeeScb"
      },
      "source": [
        "## Get the training data from the github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nikNsc0eScc",
        "outputId": "985fd1c4-978e-47e6-ebc1-72709e6c9d49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'git' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/EricKiennemann/DSTI_Python_Labs.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GozDiP5wAvtb",
        "outputId": "6488cf2e-d04e-40a2-9f6f-e33bd41e23d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 3] The system cannot find the path specified: '/content/DSTI_Python_Labs/assets/data_train'\n",
            "c:\\Users\\User\\Documents\\GitHub\\Fruit-Detection-with-Yolov4\\assets\n"
          ]
        }
      ],
      "source": [
        "cd $PATH_TO_ZIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2_9aeJdOTkqY"
      },
      "outputs": [],
      "source": [
        "!mkdir $IMAGES_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwmsY_QL4sWz"
      },
      "source": [
        "**Unzip the file training data file locally locally**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BUwZi1q0mUhY"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'thumbv3.zip'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mzipfile\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39;49mZipFile(ZIP_FILE, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m zip_ref:\n\u001b[0;32m      3\u001b[0m     zip_ref\u001b[39m.\u001b[39mextractall(IMAGES_RAW)\n",
            "File \u001b[1;32mc:\\Python310\\lib\\zipfile.py:1249\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m   1248\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1249\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mopen(file, filemode)\n\u001b[0;32m   1250\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   1251\u001b[0m         \u001b[39mif\u001b[39;00m filemode \u001b[39min\u001b[39;00m modeDict:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'thumbv3.zip'"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(ZIP_FILE, 'r') as zip_ref:\n",
        "    zip_ref.extractall(IMAGES_RAW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMVMAcL1UKSC",
        "outputId": "5b472a26-c85f-47ef-c251-10a29f276e01"
      },
      "outputs": [],
      "source": [
        "!ls $IMAGES_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj5Iz1LFeScf"
      },
      "source": [
        "## Prepare the files for the processing\n",
        "\n",
        "Split the files into three datasets (folders) :\n",
        "* \"train\" for training\n",
        "* \"valid\" for validation\n",
        "* \"test\" for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNeBPS30eScg",
        "outputId": "048ce8a5-5977-4731-b8bc-f24745ed57dc"
      },
      "outputs": [],
      "source": [
        "def TrainValidTestFruit(category):\n",
        "\t# Path to the directory where the original dataset was uncompressed\n",
        "\toriginal_dataset_dir = IMAGES_RAW\n",
        "\n",
        "\t# Directory where the three datasets will be stored\n",
        "\tbase_dir = IMAGES_SPLITED\n",
        "\n",
        "\tos.mkdir(base_dir)\n",
        "\n",
        "\t# Directory for the training splits\n",
        "\ttrain_dir = os.path.join(base_dir, 'train')\n",
        "\tos.mkdir(train_dir)\n",
        "\n",
        "\t# Directory for the validation splits\n",
        "\tvalid_dir = os.path.join(base_dir, 'valid')\n",
        "\tos.mkdir(valid_dir)\n",
        "\n",
        "\t# Directory for the test splits\n",
        "\ttest_dir = os.path.join(base_dir, 'test')\n",
        "\tos.mkdir(test_dir)\n",
        "\n",
        "\tfor cat in category:\n",
        "\t\t# Directories for training categories\n",
        "\t\ttrain_category_dir = os.path.join(train_dir, cat)\n",
        "\t\tos.mkdir(train_category_dir)\n",
        "\n",
        "\t\t# Directories for validation categories\n",
        "\t\tvalid_category_dir = os.path.join(valid_dir, cat)\n",
        "\t\tos.mkdir(valid_category_dir)\n",
        "\n",
        "\t\t# Directories for test categories\n",
        "\t\ttest_category_dir = os.path.join(test_dir, cat)\n",
        "\t\tos.mkdir(test_category_dir)\n",
        "\n",
        "\t\tdata_folder = os.path.join(original_dataset_dir, cat)\n",
        "\t\tjpgfiles = os.listdir(data_folder)\n",
        "\n",
        "\t\tnb_images = len(jpgfiles)\n",
        "\t\ttrain_ratio = 0.75\t\t\t\t# 75% of files for training\n",
        "\t\tvalidation_ratio = 0.15\t\t# 15% of files for validation\n",
        "\t\ttest_ratio = 0.10\t\t\t\t\t# 10% of files for test\n",
        "\n",
        "\t\tdataX = np.arange(nb_images)\n",
        "\t\t# train is now 75% of the entire data set\n",
        "\t\tx_train, x_test = train_test_split(dataX, test_size=1 - train_ratio)\n",
        "\n",
        "\t\t# test is now 10% of the initial data set\n",
        "\t\t# validation is now 15% of the initial data set\n",
        "\t\tx_valid, x_test = train_test_split(x_test, test_size=test_ratio/(test_ratio + validation_ratio)) \n",
        "\n",
        "\t\t# Copy the train files \n",
        "\t\tfnames = [jpgfiles[i] for i in x_train]\n",
        "\t\tfor fname in fnames:\n",
        "\t\t\tsrc = os.path.join(original_dataset_dir, cat, fname)\n",
        "\t\t\tdst = os.path.join(train_category_dir, fname)\n",
        "\t\t\tshutil.copyfile(src, dst)\n",
        "\n",
        "\t\t# Copy the validation files \n",
        "\t\tfnames = [jpgfiles[i] for i in x_valid]\n",
        "\t\tfor fname in fnames:\n",
        "\t\t\tsrc = os.path.join(original_dataset_dir, cat, fname)\n",
        "\t\t\tdst = os.path.join(valid_category_dir, fname)\n",
        "\t\t\tshutil.copyfile(src, dst)\n",
        "\n",
        "\t\t# Copy the test files \n",
        "\t\tfnames = [jpgfiles[i] for i in x_test]\n",
        "\t\tfor fname in fnames:\n",
        "\t\t\tsrc = os.path.join(original_dataset_dir, cat, fname)\n",
        "\t\t\tdst = os.path.join(test_category_dir, fname)\n",
        "\t\t\tshutil.copyfile(src, dst)\n",
        "\n",
        "\n",
        "\t\t# Sanity Check to ensure that Training, Validation and Test Folders have the expected number of images\n",
        "\n",
        "\t\tprint('Number of Images in Training Directory is {} for category {}'.format(len(os.listdir(train_category_dir)),cat))\n",
        "\t\tprint('Number of Images in Validation Directory is {} for category {}'.format(len(os.listdir(valid_category_dir)),cat))\n",
        "\t\tprint('Number of Images in Test Directory is {} for category {}'.format(len(os.listdir(test_category_dir)),cat))\n",
        "\n",
        "# Run the creation of the three datasets on our three labels\n",
        "TrainValidTestFruit([THUMB_UP,THUMB_DOWN,OTHER])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iA1o5iRZ3Xj"
      },
      "source": [
        "The dataset is quit well balanced between 'thumb up' 517 images and 'thumb down' 593 images for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTbCJ3LGeScn"
      },
      "source": [
        "## Building the Neural Network\n",
        "\n",
        "We'll be using VGG16 model and the corresponding preprocessing function for the input images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxEOSC_keScq",
        "outputId": "7b151dbf-f695-405a-cfc7-1e4c1a409b95"
      },
      "outputs": [],
      "source": [
        "# include_top=false => we only take the convolutional part not the classification part.\n",
        "# The image standard size is (224,224)\n",
        "base_model = VGG16(include_top=False, weights='imagenet', input_shape = (224,224,3))\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3RonFg9eScs"
      },
      "source": [
        "Note that we have downloaded only a convolution part of the neural network. Let's add some dense layers on top of it.  \n",
        "I choose a sigmoid activation in order to be able to dect more easelly when there is \"nothing\" in the screen. If the probability for both 'thumb up' and 'thumb down' are low it is likely that there is no thumb on the screen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCIKc-S_eScv"
      },
      "outputs": [],
      "source": [
        "flatten = Flatten()(base_model.output)\n",
        "dropout_1 = Dropout(0.25)(flatten)\n",
        "fc_1 = Dense(128)(dropout_1)\n",
        "dropout_2 = Dropout(0.5)(fc_1)\n",
        "predictions = Dense(NB_CLASSES, activation=\"sigmoid\", name='predictions')(dropout_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrKIyWb8eScx"
      },
      "outputs": [],
      "source": [
        "model = Model(base_model.input, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh-3rJpkUMQR"
      },
      "source": [
        "**The final model structure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N54508MveScz",
        "outputId": "02e3c9c1-d96b-4435-a2d2-73935bb02b7d"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jIlvemneSc2"
      },
      "source": [
        "**Chosing the optimizer parameters and compiling the model**  \n",
        "Categorical crossentropy is choosen for this multi label classification problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlN5Q7OveSc5"
      },
      "outputs": [],
      "source": [
        "loss = 'categorical_crossentropy'\n",
        "learning_rate = 0.001\n",
        "optimizer = optimizers.SGD ## optimizers.SGD \n",
        "metrics = ['accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NAfP8ULeSc7"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer(learning_rate),\n",
        "              metrics=metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsMwxsnIeSc9"
      },
      "source": [
        "## Data preparation\n",
        "We will do data augmentation in order to have more data for the training.\n",
        "We apply :\n",
        "* rotation\n",
        "* width shift\n",
        "* height shift\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNsJQ1mteSc_"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(IMAGES_SPLITED, \"train\")\n",
        "val_dir = os.path.join(IMAGES_SPLITED, \"valid\")\n",
        "test_dir = os.path.join(IMAGES_SPLITED, \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8qfSmFeSdC"
      },
      "outputs": [],
      "source": [
        "# we'll resize images in correspondance to network input size\n",
        "image_size = (224,224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NWtfjX4eSdE",
        "outputId": "e37c8a15-deb2-4716-a558-50e1d8b0ea6f"
      },
      "outputs": [],
      "source": [
        "# apply some data augmentation\n",
        "#\n",
        "train_datagen = ImageDataGenerator(rotation_range=15,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   fill_mode='nearest',\n",
        "                                   preprocessing_function=preprocess_input_vgg\n",
        "                                  )\n",
        "\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg) # for validation we don't need to augment\n",
        "\n",
        "train_batchsize = 40\n",
        "val_batchsize = 40\n",
        "\n",
        "# this function takes images from folders and feeds to Imagedatagenerator\n",
        "train_generator = train_datagen.flow_from_directory( \n",
        "        train_dir,\n",
        "        target_size=image_size,\n",
        "        batch_size=train_batchsize,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=image_size,\n",
        "        batch_size=val_batchsize,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4kh8Im0Zg9M"
      },
      "source": [
        "**The data generation is only applied to the train dataset**\n",
        "\n",
        "We do have 1370 images for training (without data augmentation) and 273 images for validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1svgQ-iHeSdI"
      },
      "source": [
        "## Model training\n",
        "\n",
        "Starting with a number of epoch equal to 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rANG4-YdeSdI"
      },
      "outputs": [],
      "source": [
        "epochs = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8sDL64PeSdL"
      },
      "outputs": [],
      "source": [
        "nb_train_steps = train_generator.samples // train_generator.batch_size\n",
        "nb_val_steps = validation_generator.samples // validation_generator.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F631Er62eSdR",
        "outputId": "4dd32753-3df2-41a2-cc35-f7d54a1451c0"
      },
      "outputs": [],
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=nb_train_steps,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=nb_val_steps,\n",
        "      verbose=1, #0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJMbTdPjkFw4"
      },
      "source": [
        "**The accuracy for training and validation dataset are good**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbCByQ09eSdT",
        "outputId": "9fef8f76-9e83-4b36-cfaf-761a54a7a071"
      },
      "outputs": [],
      "source": [
        "print('training acc.:',history.history['accuracy'][-1])\n",
        "print('val acc.:', (history.history['val_accuracy'])[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUbDqvBgeSdW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy %')\n",
        "    plt.plot(history.epoch, np.array(history.history['accuracy']),\n",
        "    label='Train Accuracy')\n",
        "    plt.plot(history.epoch, np.array(history.history['val_accuracy']),\n",
        "    label = 'Val Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy for train and validation')\n",
        "    plt.ylim([0, 1.1])\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.plot(history.epoch, np.array(history.history['loss']),\n",
        "    label='Train Loss')\n",
        "    plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
        "    label = 'Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss for train and validation')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "U_uEY2IYeSdY",
        "outputId": "ff20d38c-62b7-4694-d616-80b5ec916c61"
      },
      "outputs": [],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDHG_-PReSda"
      },
      "source": [
        "## Saving model\n",
        "\n",
        "The model is saved to be used in the back end part of the web application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJwoycSceSdb"
      },
      "outputs": [],
      "source": [
        "os.makedirs(SAVE_FOLDER, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTxpe7VqeSde"
      },
      "outputs": [],
      "source": [
        "model_path = os.path.join(SAVE_FOLDER, MODEL_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA-0riSMeSdh"
      },
      "outputs": [],
      "source": [
        "model.save(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQWvNsPseSdj"
      },
      "source": [
        "## Final test on the test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVa1mULOeSdl"
      },
      "outputs": [],
      "source": [
        "model = load_model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkuwLmLUozYh"
      },
      "source": [
        "Apply the same preprocessing on the images as for validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gHBUIT1eSdu"
      },
      "outputs": [],
      "source": [
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYvsmjhvpAQX"
      },
      "source": [
        "Realize the prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u43VYZXgeSdw",
        "outputId": "45678e95-cb58-48be-d3f4-cd08685c3ad9"
      },
      "outputs": [],
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=image_size,\n",
        "        shuffle = False,\n",
        "        class_mode='categorical',\n",
        "        batch_size=1)\n",
        "\n",
        "filenames = test_generator.filenames\n",
        "nb_samples = len(filenames)\n",
        "\n",
        "predict = model.predict(test_generator,steps=nb_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbDXkFyCpH4V"
      },
      "source": [
        "The prediction has been done for 185 images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYog3wLupaST"
      },
      "source": [
        "**See the result of the prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2_BVlX-pflo"
      },
      "outputs": [],
      "source": [
        "def show_classification_confusion_matrix(y_valid,y_fit,list_classes):\n",
        "  print(classification_report(y_valid, y_fit,\n",
        "                            target_names = list_classes))\n",
        "  mat = confusion_matrix(y_valid, y_fit)\n",
        "  sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=list_classes,\n",
        "            yticklabels=list_classes)\n",
        "  plt.xlabel('true label')\n",
        "  plt.ylabel('predicted label')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "kK1zDpFteSdy",
        "outputId": "1bfad3c3-2169-4e3a-a485-f60d13b01ca3"
      },
      "outputs": [],
      "source": [
        "# choose the higher probability as the best prediction\n",
        "y_pred = np.argmax(predict, axis=1)\n",
        "classes = [\"{:02d}\".format(i) for i in range(1, 4)]\n",
        "\n",
        "show_classification_confusion_matrix(test_generator.classes,y_pred,classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeIo27TyqazC"
      },
      "source": [
        "**All the images has been correctly predicted**\n",
        "\n",
        "**The model is kept and will be use for the web application**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3vEgReKqyUX"
      },
      "source": [
        "## Store the model file on a google account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU8KDPZGqoOo",
        "outputId": "0beaeca6-76a3-47f1-9476-e19c94bbf928"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArDwLMxzrBgO"
      },
      "outputs": [],
      "source": [
        "%cp $model_path ../../../gdrive/'My Drive'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Thumb Classification Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
